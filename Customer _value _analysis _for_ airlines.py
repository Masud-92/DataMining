# -*- coding: utf-8 -*-
"""2023124970_DM_Masud

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dSWPR81Hw4nMpbc5UaDXRE40kAZyl1iW
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn import tree
from io import StringIO
import pydotplus
from sklearn.svm import SVC  # Added SVM import

# Define utility functions for visualization and data handling
def cm_plot(y, yp):
    cm = confusion_matrix(y, yp)
    plt.matshow(cm, cmap=plt.cm.Greens)
    plt.colorbar()
    for x in range(len(cm)):
        for y in range(len(cm)):
            plt.annotate(cm[x, y], xy=(x, y), horizontalalignment='center', verticalalignment='center')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    return plt

def leida(data, kmodel):
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    labels = data.columns
    k = len(labels)
    plot_data = kmodel.cluster_centers_
    color = ['b', 'g', 'r', 'c', 'y']
    angles = np.linspace(0, 2*np.pi, k, endpoint=False)
    plot_data = np.concatenate((plot_data, plot_data[:, [0]]), axis=1)
    angles = np.concatenate((angles, [angles[0]]))
    fig = plt.figure()
    ax = fig.add_subplot(111, polar=True)
    for i in range(len(plot_data)):
        ax.plot(angles, plot_data[i], 'o-', color=color[i], label=u'客户群'+str(i), linewidth=2)
    ax.set_rgrids(np.arange(0.01, 3.5, 0.5), np.arange(-1, 2.5, 0.5), fontproperties="SimHei")
    ax.set_thetagrids(angles * 180/np.pi, labels, fontproperties="SimHei")
    plt.legend(loc=4)
    plt.show()

# Data Cleaning Function
def clean(dataFile):
    cleanedFile = './data/data_cleaned.csv'
    data = pd.read_csv(dataFile, encoding='utf-8')
    data = data[data['SUM_YR_1'].notnull() & data['SUM_YR_2'].notnull()]
    index1 = data['SUM_YR_1'] != 0
    index2 = data['SUM_YR_2'] != 0
    index3 = (data['SEG_KM_SUM'] == 0) & (data['avg_discount'] == 0)
    data = data[index1 | index2 | index3]
    data.to_csv(cleanedFile, index=False)

# Feature Selection Function
def change(fileName):
    changedFile = './data/data_changed.csv'
    data = pd.read_csv(fileName, encoding='utf-8')
    data = data[['LOAD_TIME', 'FFP_DATE', 'LAST_TO_END', 'FLIGHT_COUNT', 'avg_discount', 'SEG_KM_SUM']]
    data.to_csv(changedFile, index=False, encoding='utf-8')

# Compute LRFMC Metrics
def LRFMC(fileName):
    data = pd.read_csv(fileName)
    data2 = pd.DataFrame(columns=['L', 'R', 'F', 'M', 'C'])
    time_list = []
    import datetime
    for i in range(len(data['LOAD_TIME'])):
        str1 = data['LOAD_TIME'][i].split('/')
        str2 = data['FFP_DATE'][i].split('/')
        temp = datetime.datetime(int(str1[0]), int(str1[1]), int(str1[2])) - datetime.datetime(int(str2[0]), int(str2[1]), int(str2[2]))
        time_list.append(temp.days)
    data2['L'] = pd.Series(time_list)
    data2['R'] = data['LAST_TO_END']
    data2['F'] = data['FLIGHT_COUNT']
    data2['M'] = data['SEG_KM_SUM']
    data2['C'] = data['avg_discount']
    data2.to_csv('./data/data_changed2.csv', index=False, encoding='utf-8')

# Standardization Function
def standard():
    data = pd.read_csv('./data/data_changed2.csv', encoding='utf-8').iloc[:, 1:]
    zscoredfile = './data/data_standard.csv'
    data = (data - data.mean(axis=0)) / (data.std(axis=0))
    data.columns = ['Z' + i for i in data.columns]
    data.to_csv(zscoredfile, index=False)

# Run Data Cleaning and Preprocessing
dataFile = './data/air_data.csv'
#clean()
clean(dataFile)
change('./data/data_cleaned.csv')
LRFMC('./data/data_changed.csv')
standard()

# Load the standardized dataset
data = pd.read_csv('./data/data_standard.csv')
data_values = data.values

# Visualize Clusters using Radar Chart
#leida(data, kmodel)
# Modified leida function
def leida(data, kmodel):

    labels = ['ZL', 'ZR', 'ZF', 'ZM']  # Update labels to match number of clusters


    ax.set_thetagrids(angles * 180/np.pi, labels, fontproperties="SimHei")  # Use updated labels

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the standardized dataset
data = pd.read_csv('./data/data_standard.csv')
data_values = data.values

# Apply K-Means Clustering
k = 5  # Number of clusters
kmodel = KMeans(n_clusters=k, n_init=10)  # Add n_init=10 to suppress the warning
kmodel.fit(data_values)

# Get the cluster labels and centroids
labels = kmodel.labels_
centroids = kmodel.cluster_centers_

# Create a scatter plot to visualize the clusters
plt.figure(figsize=(10, 7))
colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

for i in range(k):
    # Select data points that belong to the i-th cluster
    cluster_data = data_values[labels == i]

    # Plot the data points
    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], c=colors[i % len(colors)], label=f'Cluster {i}')

    # Plot the centroids
    plt.scatter(centroids[i, 0], centroids[i, 1], c='black', marker='x', s=100)

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('K-Means Clustering')
plt.legend()
plt.show()

def plot_elbow_method(data, max_clusters=10):
    wcss = [] # Within-cluster sum of squares
    for i in range(1, max_clusters + 1):
        kmeans = KMeans(n_clusters=i, random_state=0)
        kmeans.fit(data)
        wcss.append(kmeans.inertia_) # Inertia is the sum of squared distances to the nearest cluster center
    plt.plot(range(1, max_clusters + 1), wcss, marker='o')
    plt.title('Elbow Method for Optimal k')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Function to plot the elbow method
def plot_elbow_method(data):
    wcss = []
    for i in range(1, 11):
        kmeans = KMeans(n_clusters=i, n_init=10)
        kmeans.fit(data)
        wcss.append(kmeans.inertia_)
    plt.figure(figsize=(10, 7))
    plt.plot(range(1, 11), wcss, marker='o')
    plt.title('Elbow Method For Optimal Number of Clusters')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')  # Within-cluster sum of squares
    plt.show()

# Load the standardized dataset
data = pd.read_csv('./data/data_standard.csv')

# Plot the elbow method to find the optimal number of clusters
plot_elbow_method(data)

# Choose the number of clusters based on the elbow plot
optimal_clusters = 4  # Adjust based on the elbow plot

# Instantiate and fit the KMeans model directly
kmeans = KMeans(n_clusters=optimal_clusters, n_init=10)  # Use n_init=10 to suppress the warning
kmeans.fit(data)

# Add the cluster labels to the data
data['Cluster'] = kmeans.labels_

# Plot the clusters
plt.figure(figsize=(10, 7))
colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']
# Get column names for plotting
x_column = data.columns[0] # Assuming first column is the x-axis
y_column = data.columns[1] # Assuming second column is the y-axis

for cluster in range(optimal_clusters):
    cluster_data = data[data['Cluster'] == cluster]
    # Use the identified column names for plotting
    plt.scatter(cluster_data[x_column], cluster_data[y_column], label=f'Cluster {cluster}', color=colors[cluster % len(colors)])
plt.xlabel(x_column) # Set x-axis label
plt.ylabel(y_column) # Set y-axis label
plt.legend()
plt.title('Clusters of Standardized Data')
plt.show()

# Exploratory Data Analysis
data_explore = pd.read_csv(dataFile, encoding='utf-8')
explore = data_explore.describe(percentiles=[], include='all').T
explore['null'] = len(data_explore) - explore['count']
explore = explore[['null', 'max', 'min']]
explore.columns = [u'空值数', u'最大值', u'最小值']
# Remove the encoding argument and let pandas handle encoding automatically
explore.to_excel('./data/explore.xlsx')

# Train SVM Classifier
train_data, test_data, train_label, test_label = train_test_split(data_values, kmodel.labels_, test_size=0.2)
clf = SVC(kernel='linear', C=1.0)  # Using linear kernel for simplicity
clf.fit(train_data, train_label)

# Predict labels using the trained SVM model
predicted_labels_svm = clf.predict(test_data)  # Add this line to predict labels

# Evaluate the SVM Classifier
predicted_labels = clf.predict(test_data)

# Print Classification Report
print("Classification Report:\n", classification_report(test_label, predicted_labels))

# Plot Confusion Matrix
cm_plot(test_label, predicted_labels).show()





# Train Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
train_data, test_data, train_label, test_label = train_test_split(data_values, kmodel.labels_, test_size=0.2)
clf_tree = DecisionTreeClassifier()
clf_tree.fit(train_data, train_label)

# Evaluate the Decision Tree Classifier
predicted_labels_tree = clf_tree.predict(test_data)

# Print Classification Report
print("Decision Tree Classification Report:\n", classification_report(test_label, predicted_labels_tree))

# Plot Confusion Matrix
cm_plot(test_label, predicted_labels_tree).show()

# Visualize the Decision Tree
from google.colab import files # Import the 'files' object from google.colab
from io import StringIO
import pydotplus
from sklearn import tree

dot_data = StringIO()
tree.export_graphviz(clf_tree, out_file=dot_data, filled=True, rounded=True, special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png("decision_tree.png")
files.download("decision_tree.png") # Now you can use 'files.download'

